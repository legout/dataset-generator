# S3 Integration

Dataset Generator supports S3-compatible storage for scalable, cloud-native data generation.

## Supported Storage Services

- **Amazon S3** - Primary cloud storage
- **MinIO** - Self-hosted S3-compatible storage
- **Wasabi** - S3-compatible alternative
- **DigitalOcean Spaces** - S3-compatible object storage
- **Any S3-compatible service** - Generic endpoint support

## Configuration

### Basic S3 Setup

```python
from dataset_generator import create_writer, S3Config

s3_config = S3Config(
    uri="s3://my-bucket",
    key="your-access-key",
    secret="your-secret-key",
    endpoint_url="https://s3.amazonaws.com",
    region="us-west-2",
)

writer = create_writer(
    "parquet",
    output_uri="s3://my-bucket/data",
    s3=s3_config,
)
```

### Environment Variables

```bash
# AWS S3
export AWS_ACCESS_KEY_ID=your-access-key
export AWS_SECRET_ACCESS_KEY=your-secret-key
export AWS_DEFAULT_REGION=us-west-2

# Alternative S3 services
export S3_ACCESS_KEY=your-access-key
export S3_SECRET_KEY=your-secret-key
export S3_ENDPOINT=https://s3.amazonaws.com
export S3_REGION=us-west-2
```

## Service-Specific Setup

### Amazon S3

```python
s3_config = S3Config(
    uri="s3://my-data-bucket",
    key=os.getenv("AWS_ACCESS_KEY_ID"),
    secret=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region="us-west-2",
)
```

### MinIO (Local)

```python
s3_config = S3Config(
    uri="s3://dataset-bucket",
    key="minioadmin",
    secret="minioadmin",
    endpoint_url="http://localhost:9000",
    region="us-east-1",
)
```

### MinIO (Docker Compose)

```yaml
# docker-compose.yml
version: '3.8'
services:
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

volumes:
  minio_data:
```

### Wasabi

```python
s3_config = S3Config(
    uri="s3://my-wasabi-bucket",
    key="wasabi-access-key",
    secret="wasabi-secret-key",
    endpoint_url="https://s3.us-west-1.wasabisys.com",
    region="us-west-1",
)
```

## Usage Examples

### CLI with S3

```bash
# Basic S3 output
dataset-generator generate ecommerce \
  --format parquet \
  --output s3://my-bucket/ecommerce \
  --s3-uri s3://my-bucket \
  --s3-key $AWS_ACCESS_KEY_ID \
  --s3-secret $AWS_SECRET_ACCESS_KEY \
  --s3-region us-west-2

# MinIO output
dataset-generator generate ecommerce \
  --format delta \
  --output s3://datasets/ecommerce \
  --s3-uri s3://datasets \
  --s3-key minioadmin \
  --s3-secret minioadmin \
  --s3-endpoint http://localhost:9000
```

### Python API

```python
from dataset_generator import create_generator, create_writer, S3Config, write_dataset

# Create generator
generator = create_generator(
    "ecommerce",
    n_customers=100_000,
    orders_per_day=10_000,
    start_date=date(2024, 1, 1),
    end_date=date(2024, 1, 31),
)

# Configure S3
s3_config = S3Config(
    uri="s3://my-bucket",
    key="access-key",
    secret="secret-key",
    endpoint_url="https://s3.amazonaws.com",
    region="us-west-2",
)

# Create writer
writer = create_writer(
    "parquet",
    output_uri="s3://my-bucket/ecommerce-data",
    s3=s3_config,
    options=WriterOptions(
        file_rows_target=1_000_000,
        compression="snappy",
    ),
)

# Generate data
write_dataset(generator, writer)
```

## Performance Optimization

### Multipart Uploads

Dataset Generator automatically uses S3 multipart uploads for large files:

```python
# Optimize for large files
WriterOptions(file_rows_target=2_000_000)
```

### Parallel Processing

```python
# Multiple writers can work in parallel
writers = [
    create_writer("parquet", output_uri=f"s3://bucket/part-{i}", s3=s3_config)
    for i in range(4)
]
```

### Network Optimization

```python
# Use regional endpoints for better performance
s3_config = S3Config(
    endpoint_url="https://s3.us-west-2.amazonaws.com",  # Regional endpoint
    region="us-west-2",
)
```

## Security Best Practices

### IAM Roles (Recommended)

```python
# When running on EC2/ECS, use IAM roles
s3_config = S3Config(
    uri="s3://my-bucket",
    # No key/secret needed - uses instance profile
    region="us-west-2",
)
```

### Access Keys

```python
# Use environment variables, not hardcoded keys
import os

s3_config = S3Config(
    uri="s3://my-bucket",
    key=os.getenv("AWS_ACCESS_KEY_ID"),
    secret=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region=os.getenv("AWS_DEFAULT_REGION"),
)
```

### Bucket Policies

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/data-generator"
      },
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::my-bucket",
        "arn:aws:s3:::my-bucket/*"
      ]
    }
  ]
}
```

## Troubleshooting

### Connection Issues

```python
# Test S3 connectivity
import s3fs
fs = s3fs.S3FileSystem(
    key="your-key",
    secret="your-secret",
    client_kwargs={'region_name': 'us-west-2'}
)

try:
    fs.ls("s3://my-bucket")
    print("S3 connection successful")
except Exception as e:
    print(f"S3 connection failed: {e}")
```

### Common Errors

**Access Denied**
- Check IAM permissions
- Verify bucket exists
- Validate access keys

**Endpoint Connection Error**
- Verify endpoint URL
- Check network connectivity
- Confirm region is correct

**Timeout Errors**
- Increase timeout settings
- Check network stability
- Use regional endpoints

### Debug Mode

```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Create writer with debug info
writer = create_writer("parquet", output_uri="s3://bucket/data", s3=s3_config)
```

## Cost Optimization

### Storage Class Selection

```python
# For infrequently accessed data
s3_config = S3Config(
    uri="s3://my-bucket",
    key="access-key",
    secret="secret-key",
    client_kwargs={
        'region_name': 'us-west-2',
        'config': {
            's3': {'addressing_style': 'path'}
        }
    }
)
```

### Lifecycle Policies

```json
{
  "Rules": [
    {
      "ID": "DataLifecycle",
      "Status": "Enabled",
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        }
      ]
    }
  ]
}
```

## Best Practices

1. **Use IAM roles** instead of access keys when possible
2. **Enable encryption** for sensitive data
3. **Set appropriate bucket policies** for security
4. **Monitor costs** with S3 usage metrics
5. **Use regional endpoints** for better performance
6. **Implement lifecycle policies** for cost optimization

## Next Steps

- **[Configuration](configuration.md)** - Advanced configuration options
- **[Writer Formats](writers/parquet.md)** - Choose output format
- **[Examples](../examples/jupyter.md)** - Complete working examples